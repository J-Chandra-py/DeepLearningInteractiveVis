{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data from Excel sheet\n",
    "import pandas as pd\n",
    "df = pd.read_excel('hippocampus_volume_relevance_DELCODE.xlsx', sheet_name='DELCODE_LRP_CMP')\n",
    "#print(df)\n",
    "sid = df['SID']\n",
    "grp = df['prmdiag']\n",
    "age = df['age']\n",
    "sex = df['sex_bin_1female']\n",
    "tiv = df['TIV_CAT12']\n",
    "field = df['FieldStrength']\n",
    "amybin = df['ratio_Abeta42_40_pos']\n",
    "grpbin = (grp > 0) # 0=CN, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100, 100, 120, 1)\n"
     ]
    }
   ],
   "source": [
    "# load sample scan from CN class\n",
    "import glob\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "# define FOV to reduce required memory size\n",
    "x_range_from = 10; x_range_to = 110\n",
    "y_range_from = 13; y_range_to = 133\n",
    "z_range_from = 5; z_range_to = 105\n",
    "fname = glob.glob('mwp1_CAT12_DELCODE/0_CN/mwp1mprage_ca8cd6a27M0_T1_01_1.nii*')[0]\n",
    "nifti = nib.load(fname)\n",
    "data = nifti.get_fdata()\n",
    "# prepare image\n",
    "img = data[x_range_from:x_range_to, y_range_from:y_range_to, z_range_from:z_range_to]\n",
    "img = np.transpose(img, (2, 0, 1))  # reorder dimensions to match coronal view z*x*y in MRIcron etc.\n",
    "img = np.flip(img)  # flip all positions\n",
    "img = np.expand_dims(img, axis=(0,4))\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching covariates for loaded files ...\n",
      "Checking for scans not found in Excel sheet:  1\n",
      "      Age  Sex   TIV  FieldStrength\n",
      "473  75.6    0  1446              3\n"
     ]
    }
   ],
   "source": [
    "# Match covariate information\n",
    "import re\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "debug = False\n",
    "cov_idx = [-1]\n",
    "print('Matching covariates for loaded files ...')\n",
    "for i,id in enumerate(sid):\n",
    "  p = [j for j,x in enumerate(fname) if re.search('_%s' % id, x)] # extract ID numbers from filename, translate to Excel row index\n",
    "  if len(p)==0:\n",
    "    if debug: print('Did not find %04d' % id) # did not find Excel sheet subject ID in loaded file selection\n",
    "  else:\n",
    "    if debug: print('Found %04d in %s: %s' % (id, p[0], fname))\n",
    "    cov_idx[p[0]] = i # store Excel index i for data file index p[0]\n",
    "print('Checking for scans not found in Excel sheet: ', sum(x<0 for x in cov_idx))\n",
    "\n",
    "labels = pd.DataFrame({'Group':grpbin}).iloc[cov_idx, :]\n",
    "labels = to_categorical(np.asarray(labels)) # use grps to access original labels\n",
    "grps = pd.DataFrame({'Group':grp, 'RID':sid}).iloc[cov_idx, :]\n",
    "\n",
    "\n",
    "# Perform regression-based covariates cleaning\n",
    "from sklearn import linear_model\n",
    "from pandas import DataFrame\n",
    "\n",
    "covariates = DataFrame({'Age':age, 'Sex':sex, 'TIV':tiv, 'FieldStrength':field}).iloc[cov_idx, :]\n",
    "print(covariates.head())\n",
    "covariates = covariates.to_numpy(dtype=np.float32) # convert dataframe to nparray with 32bit types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load linear model coefficients\n",
    "import h5py\n",
    "hf = h5py.File('linear_models_ADNI2.hdf5', 'r')\n",
    "hf.keys  # read keys\n",
    "lmarray = np.array(hf.get('linearmodels'), dtype=np.float32)  # stores 4 coefficients + 1 intercept per voxel\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "residualizing slice z= 0\n",
      "residualizing slice z= 10\n",
      "residualizing slice z= 20\n",
      "residualizing slice z= 30\n",
      "residualizing slice z= 40\n",
      "residualizing slice z= 50\n",
      "residualizing slice z= 60\n",
      "residualizing slice z= 70\n",
      "residualizing slice z= 80\n",
      "residualizing slice z= 90\n",
      "residualizing slice z= 100\n",
      "residualizing slice z= 110\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "residuals = np.copy(img)\n",
    "# create a second version of the residuals for lowered itensities (occluded image)\n",
    "residuals_occluded = np.multiply(img, 0.5) # reduce local intensity/volume by 50%, i.e. 0.46 -> 0.23\n",
    "\n",
    "lm = linear_model.LinearRegression()\n",
    "for k in range(img.shape[3]):\n",
    "    if (k % 10 == 0): print('residualizing slice z=', str(k))\n",
    "    for j in range(img.shape[2]):\n",
    "        for i in range(img.shape[1]):\n",
    "            if any(lmarray[k, j, i, :] != 0): # skip empty voxels/space\n",
    "                # load fitted linear model coefficients\n",
    "                lm.coef_ = lmarray[k, j, i, :4]\n",
    "                lm.intercept_ = lmarray[k, j, i, 4]\n",
    "                pred = lm.predict(covariates)  # calculate prediction for all subjects\n",
    "                residuals[0, i, j, k, 0] = residuals[0, i, j, k, 0] - pred  # % subtract effect of covariates from original values (=calculate residuals)\n",
    "                residuals_occluded[0, i, j, k, 0] = residuals_occluded[0, i, j, k, 0] - pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "displaying residual image  mwp1_CAT12_DELCODE/0_CN\\mwp1mprage_ca8cd6a27M0_T1_01_1.nii.gz\n"
     ]
    }
   ],
   "source": [
    "# Display a single scan (residuals)\n",
    "from matplotlib import pyplot as plt\n",
    "#import numpy as np\n",
    "test_img = residuals[0, :,:,:, 0] # img[0, :,:,:, 0]\n",
    "ma = np.max(test_img)\n",
    "mi = np.min(test_img)\n",
    "test_img = (test_img - mi) / (ma - mi) # normalising to (0-1) range\n",
    "#test_img = (test_img - test_img.mean())/test_img.std() # normalizing by mean and sd\n",
    "print('displaying residual image ', fname)\n",
    "for i in range(test_img.shape[2]):\n",
    "  if (i % 25 == 0): # only display each xxth slice\n",
    "    plt.figure()\n",
    "    a = test_img[:,:,i]\n",
    "    plt.imshow(a, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.4\n"
     ]
    }
   ],
   "source": [
    "# Load CNN model from disk\n",
    "# specify version of tensorflow\n",
    "#%tensorflow_version 1.x  # <- use this for Google colab\n",
    "import tensorflow as tf\n",
    "# downgrade to specific version\n",
    "#!pip install tensorflow-gpu==1.15\n",
    "#import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "# disable tensorflow deprecation warnings\n",
    "import logging\n",
    "logging.getLogger('tensorflow').disabled=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_1 (Conv3D)            (None, 100, 100, 120, 5)  140       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 50, 50, 60, 5)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 50, 50, 60, 5)     20        \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 50, 50, 60, 5)     680       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 25, 25, 30, 5)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 25, 25, 30, 5)     20        \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 25, 25, 30, 5)     680       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3 (None, 12, 12, 15, 5)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 12, 12, 15, 5)     20        \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 10800)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 10800)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                691264    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 694,970\n",
      "Trainable params: 694,940\n",
      "Non-trainable params: 30\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model, Model\n",
    "import innvestigate\n",
    "import innvestigate.utils as iutils\n",
    "import os\n",
    "\n",
    "mymodel = load_model('model_checkpoints/resmodel_wb_whole_ds.hdf5')\n",
    "\n",
    "# see https://github.com/albermax/innvestigate/blob/master/examples/notebooks/imagenet_compare_methods.ipynb for a list of alternative methods\n",
    "methods = [ # tuple with method,     params,                  label\n",
    "#            (\"deconvnet\",            {},                      \"Deconvnet\"),\n",
    "#            (\"guided_backprop\",      {},                      \"Guided Backprop\"),\n",
    "#            (\"deep_taylor.bounded\",  {\"low\": -1, \"high\": 1},  \"DeepTaylor\"),\n",
    "#            (\"input_t_gradient\",     {},                      \"Input * Gradient\"),\n",
    "#            (\"lrp.z\",                {},                      \"LRP-Z\"),\n",
    "#            (\"lrp.epsilon\",          {\"epsilon\": 1},          \"LRP-epsilon\"),\n",
    "#            (\"lrp.alpha_1_beta_0\",   {\"neuron_selection_mode\":\"index\"},     \"LRP-alpha1beta0\"),\n",
    "        (\"lrp.sequential_preset_a\", {\"neuron_selection_mode\": \"index\", \"epsilon\": 1e-10}, \"LRP-CMPalpha1beta0\"), # LRP CMP rule taken from https://github.com/berleon/when-explanations-lie/blob/master/when_explanations_lie.py\n",
    "]\n",
    "\n",
    "#model_wo_softmax = iutils.keras.graph.model_wo_softmax(mymodel)  ## sometimes raises: ValueError: The name \"dense_1\" is used 2 times in the model. All layer names should be unique.\n",
    "#model_wo_softmax = Model(inputs=mymodel.inputs,\n",
    "#                          outputs=iutils.keras.graph.pre_softmax_tensors(mymodel.outputs),\n",
    "#                          name=(mymodel.name + '_wo_softmax')) \n",
    "#model_wo_softmax.summary()\n",
    "mymodel.layers[-1].activation=tf.keras.activations.linear\n",
    "mymodel.save('tmp_wo_softmax.hdf5')\n",
    "model_wo_softmax = load_model('tmp_wo_softmax.hdf5')\n",
    "os.remove('tmp_wo_softmax.hdf5')\n",
    "model_wo_softmax.summary()\n",
    "\n",
    "# create analyzer\n",
    "analyzers = []\n",
    "for method in methods:\n",
    "    analyzer = innvestigate.create_analyzer(method[0], model_wo_softmax, **method[1])\n",
    "    # Some analyzers require training.\n",
    "    #   analyzer.fit(test_img, batch_size=20, verbose=1)\n",
    "    #  analyzers.append(analyzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define resolution for occlusion analysis\n",
    "step_size = 3 # should be an odd number\n",
    "half_step_size = np.floor(step_size/2).astype(int)\n",
    "radius = 10 # evaluate 15, 10, 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing slice  2  of  120\n",
      "processing slice  5  of  120\n",
      "processing slice  8  of  120\n",
      "processing slice  11  of  120\n",
      "processing slice  14  of  120\n",
      "processing slice  17  of  120\n",
      "processing slice  20  of  120\n",
      "processing slice  23  of  120\n",
      "processing slice  26  of  120\n",
      "processing slice  29  of  120\n",
      "processing slice  32  of  120\n",
      "processing slice  35  of  120\n",
      "processing slice  38  of  120\n",
      "processing slice  41  of  120\n",
      "processing slice  44  of  120\n",
      "processing slice  47  of  120\n",
      "processing slice  50  of  120\n",
      "processing slice  53  of  120\n",
      "processing slice  56  of  120\n",
      "processing slice  59  of  120\n",
      "processing slice  62  of  120\n",
      "processing slice  65  of  120\n",
      "processing slice  68  of  120\n",
      "processing slice  71  of  120\n",
      "processing slice  74  of  120\n",
      "processing slice  77  of  120\n",
      "processing slice  80  of  120\n",
      "processing slice  83  of  120\n",
      "processing slice  86  of  120\n",
      "processing slice  89  of  120\n",
      "processing slice  92  of  120\n",
      "processing slice  95  of  120\n",
      "processing slice  98  of  120\n",
      "processing slice  101  of  120\n",
      "processing slice  104  of  120\n",
      "processing slice  107  of  120\n",
      "processing slice  110  of  120\n",
      "processing slice  113  of  120\n",
      "processing slice  116  of  120\n",
      "processing slice  119  of  120\n"
     ]
    }
   ],
   "source": [
    "prediction_map = np.zeros(img.shape, dtype=np.float32) # store class probabilities for MCI/AD\n",
    "pos_relevance_map = np.zeros(img.shape, dtype=np.float32) # store total relevance for MCI/AD\n",
    "neg_relevance_map = np.zeros(img.shape, dtype=np.float32) # store total relevance for MCI/AD\n",
    "\n",
    "for k in range(half_step_size, img.shape[3]-half_step_size, step_size):\n",
    "    print('processing slice ', str(k+1), ' of ', str(img.shape[3]))\n",
    "    for j in range(half_step_size, img.shape[2]-half_step_size, step_size):\n",
    "        for i in range(half_step_size, img.shape[1]-half_step_size, step_size):\n",
    "            fov_x_start = max(0,i-radius)\n",
    "            fov_x_end = min(i+radius, img.shape[1])\n",
    "            fov_y_start = max(0,j-radius)\n",
    "            fov_y_end = min(j+radius, img.shape[2])\n",
    "            fov_z_start = max(0,k-radius)\n",
    "            fov_z_end = min(k+radius, img.shape[3])\n",
    "            \n",
    "            new_data = np.copy(residuals)\n",
    "            new_data[0, fov_x_start:fov_x_end, \n",
    "                      fov_y_start:fov_y_end ,\n",
    "                      fov_z_start:fov_z_end, 0] = residuals_occluded[0, fov_x_start:fov_x_end, \n",
    "                                                      fov_y_start:fov_y_end ,\n",
    "                                                      fov_z_start:fov_z_end, 0]\n",
    "            \n",
    "            prob = mymodel.predict(new_data)[0,1] # only store prob of AD/MCI\n",
    "            tmp = np.full([step_size, step_size, step_size], prob)\n",
    "            prediction_map[0, (i-half_step_size):(i+half_step_size+1), \n",
    "                           (j-half_step_size):(j+half_step_size+1), \n",
    "                           (k-half_step_size):(k+half_step_size+1),\n",
    "                           0] = tmp\n",
    "            \n",
    "            a = analyzer.analyze(new_data, neuron_selection=1)\n",
    "            apos = np.maximum(a, 0) # remove negative relevance\n",
    "            tmp = np.full([step_size, step_size, step_size], np.sum(apos))\n",
    "            pos_relevance_map[0, (i-half_step_size):(i+half_step_size+1), \n",
    "                           (j-half_step_size):(j+half_step_size+1), \n",
    "                           (k-half_step_size):(k+half_step_size+1),\n",
    "                           0] = tmp\n",
    "            aneg = np.minimum(a, 0) # remove positive relevance\n",
    "            tmp = np.full([step_size, step_size, step_size], np.sum(aneg))\n",
    "            neg_relevance_map[0, (i-half_step_size):(i+half_step_size+1), \n",
    "                           (j-half_step_size):(j+half_step_size+1), \n",
    "                           (k-half_step_size):(k+half_step_size+1),\n",
    "                           0] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save output as nifti images\n",
    "\n",
    "a = np.copy(prediction_map)\n",
    "a = np.squeeze(a) # remove first subject index and last color channel again\n",
    "a = np.flip(a) # flip all positions\n",
    "a = np.transpose(a, (1, 2, 0)) # reorder dimensions from coronal view z*x*y back to x*y*z\n",
    "new_data = np.zeros(data.shape, dtype=np.float32)\n",
    "new_data[x_range_from:x_range_to, y_range_from:y_range_to, z_range_from:z_range_to] = a\n",
    "out_nifti_pm = nib.Nifti1Image(new_data, nifti.affine, nifti.header)\n",
    "out_nifti_pm.to_filename('sensitivity_analysis_prediction_map_step%d_radius%d.nii.gz' % (step_size, radius))\n",
    "\n",
    "a = np.copy(pos_relevance_map)\n",
    "a = np.squeeze(a) # remove first subject index and last color channel again\n",
    "a = np.flip(a) # flip all positions\n",
    "a = np.transpose(a, (1, 2, 0)) # reorder dimensions from coronal view z*x*y back to x*y*z\n",
    "new_data = np.zeros(data.shape, dtype=np.float32)\n",
    "new_data[x_range_from:x_range_to, y_range_from:y_range_to, z_range_from:z_range_to] = a\n",
    "out_nifti_rm = nib.Nifti1Image(new_data, nifti.affine, nifti.header)\n",
    "out_nifti_rm.to_filename('sensitivity_analysis_relevance_map_pos_step%d_radius%d.nii.gz' % (step_size, radius))\n",
    "\n",
    "a = np.copy(neg_relevance_map)\n",
    "a = np.squeeze(a) # remove first subject index and last color channel again\n",
    "a = np.flip(a) # flip all positions\n",
    "a = np.transpose(a, (1, 2, 0)) # reorder dimensions from coronal view z*x*y back to x*y*z\n",
    "new_data = np.zeros(data.shape, dtype=np.float32)\n",
    "new_data[x_range_from:x_range_to, y_range_from:y_range_to, z_range_from:z_range_to] = a\n",
    "out_nifti_rm = nib.Nifti1Image(new_data, nifti.affine, nifti.header)\n",
    "out_nifti_rm.to_filename('sensitivity_analysis_relevance_map_neg_step%d_radius%d.nii.gz' % (step_size, radius))\n",
    "\n",
    "a = np.add(pos_relevance_map, neg_relevance_map)\n",
    "a = np.squeeze(a) # remove first subject index and last color channel again\n",
    "a = np.flip(a) # flip all positions\n",
    "a = np.transpose(a, (1, 2, 0)) # reorder dimensions from coronal view z*x*y back to x*y*z\n",
    "new_data = np.zeros(data.shape, dtype=np.float32)\n",
    "new_data[x_range_from:x_range_to, y_range_from:y_range_to, z_range_from:z_range_to] = a\n",
    "out_nifti_rm = nib.Nifti1Image(new_data, nifti.affine, nifti.header)\n",
    "out_nifti_rm.to_filename('sensitivity_analysis_relevance_map_step%d_radius%d.nii.gz' % (step_size, radius))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
